{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "006f5223",
   "metadata": {},
   "source": [
    "# Transformation de base\n",
    "Les transformations sont au cœur de la capacité de traitement des données de Spark. Lorsque vous appliquez une transformation à un RDD, vous créez un nouveau RDD. Les transformations sont paresseuses, ce qui signifie qu’elles ne calculent pas leurs résultats immédiatement. Au lieu de cela, ils se souviennent simplement des transformations appliquées à un ensemble de données de base (par exemple, un fichier). Les transformations ne sont calculées que lorsqu'une action nécessite qu'un résultat soit renvoyé au programme pilote.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76190a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e4d7387",
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPySparkRuntimeError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m conf \u001b[38;5;241m=\u001b[39m SparkConf()\u001b[38;5;241m.\u001b[39msetMaster(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msetAppName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransformation Notebook\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\anaconda3\\envs\\pyspark_env\\lib\\site-packages\\pyspark\\core\\context.py:523\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 523\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    524\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\anaconda3\\envs\\pyspark_env\\lib\\site-packages\\pyspark\\core\\context.py:205\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    203\u001b[0m     )\n\u001b[1;32m--> 205\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[0;32m    208\u001b[0m         master,\n\u001b[0;32m    209\u001b[0m         appName,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    219\u001b[0m         memory_profiler_cls,\n\u001b[0;32m    220\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\anaconda3\\envs\\pyspark_env\\lib\\site-packages\\pyspark\\core\\context.py:444\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[1;32m--> 444\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\anaconda3\\envs\\pyspark_env\\lib\\site-packages\\pyspark\\java_gateway.py:111\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    108\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[1;32m--> 111\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[0;32m    112\u001b[0m         errorClass\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJAVA_GATEWAY_EXITED\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    113\u001b[0m         messageParameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[0;32m    114\u001b[0m     )\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[0;32m    117\u001b[0m     gateway_port \u001b[38;5;241m=\u001b[39m read_int(info)\n",
      "\u001b[1;31mPySparkRuntimeError\u001b[0m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "source": [
    "conf = SparkConf().setMaster(\"local\").setAppName(\"Transformation Notebook\")\n",
    "sc = SparkContext.getOrCreate(conf = conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13de7c92",
   "metadata": {},
   "source": [
    "## map\n",
    "La transformation map applique une fonction à chaque élément du RDD et renvoie un nouveau RDD représentant les résultats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb890e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.parallelize((1, 2, 3, 4))\n",
    "squaredData = data.map(lambda x: x * x)\n",
    "squaredData.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85204d1",
   "metadata": {},
   "source": [
    "## filter\n",
    "La filter permet de sélectionner des éléments du RDD qui répondent à certains critères."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5765b2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.parallelize((1, 2, 3, 4))\n",
    "evenData = data.filter(lambda x : x % 2 == 0)\n",
    "evenData.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce304737",
   "metadata": {},
   "source": [
    "## flatMap\n",
    "Tout en map une fonction à chaque élément individuellement, flatMap peut produire plusieurs éléments de sortie pour chaque élément d'entrée\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54fb2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsList = sc.parallelize((\"hello world\", \"how are you\"))\n",
    "words = wordsList.flatMap(lambda x : x.split(\" \"))\n",
    "words.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f430d3b7",
   "metadata": {},
   "source": [
    "## union\n",
    "La union fusionne deux RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb5a5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize((1, 2, 3))\n",
    "rdd2 = sc.parallelize((4, 5, 6))\n",
    "commonRDD = rdd1.union(rdd2)\n",
    "commonRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b2b94c",
   "metadata": {},
   "source": [
    "## intersection\n",
    "La intersection renvoie un nouveau RDD qui contient uniquement les éléments trouvés dans les deux RDD sources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e9f687",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize((1, 2, 3))\n",
    "rdd2 = sc.parallelize((3, 4, 5))\n",
    "commonRDD = rdd1.intersection(rdd2)\n",
    "commonRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5c1b7c",
   "metadata": {},
   "source": [
    "## distinct\n",
    "distinct supprime les doublons dans un RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204a9f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize((1, 2, 3, 2, 3))\n",
    "distinctRDD = rdd.distinct()\n",
    "distinctRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e2d21a",
   "metadata": {},
   "source": [
    "## substract\n",
    "subtract renvoie un RDD avec des éléments d'un RDD qui ne sont pas trouvés dans un autre. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf44504",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize((1, 2, 3, 4))\n",
    "rdd2 = sc.parallelize((3, 4, 5, 6))\n",
    "resultRDD = rdd1.subtract(rdd2)\n",
    "resultRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8d733a",
   "metadata": {},
   "source": [
    "## cartesian\n",
    "cartesian renvoie toutes les paires possibles de (a, b) où a est dans un RDD et b est dans l'autre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c4c458",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize((1, 2))\n",
    "rdd2 = sc.parallelize((\"a\", \"b\"))\n",
    "cartesianRDD = rdd1.cartesian(rdd2)\n",
    "cartesianRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7785415",
   "metadata": {},
   "source": [
    "# Transformations cle-valeur\n",
    "Lorsque vous travaillez avec des RDD de paires clé-valeur, il existe des transformations supplémentaires qui permettent des agrégations complexes et d'autres opérations sur des paires en fonction de leur clé.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126668a7",
   "metadata": {},
   "source": [
    "## reduceByKey\n",
    "reduceByKey agrège les valeurs de chaque clé, à l'aide d'une fonction de réduction associative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acec915b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.parallelize([(\"apple\", 2), (\"orange\", 3), (\"apple\", 1)])\n",
    "reducedData = data.reduceByKey(lambda a, b: a+b)\n",
    "reducedData.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d290e3b",
   "metadata": {},
   "source": [
    "## groupByKey\n",
    "groupByKey regroupe les valeurs de chaque clé du RDD en une seule séquence. Notez qu’il reduceByKeys’agit souvent d’un meilleur choix car il présente de meilleures caractéristiques de performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9975241",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.parallelize([(\"apple\", 2), (\"orange\", 3), (\"apple\", 1)])\n",
    "groupedData = data.groupByKey()\n",
    "groupedData.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3da5619",
   "metadata": {},
   "source": [
    "## sortByKey\n",
    "sortByKey trie le RDD par clé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e498e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.parallelize([(\"apple\", 2), (\"orange\", 3), (\"banana\", 1)])\n",
    "sortedData = data.sortByKey()\n",
    "sortedData.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414901b3",
   "metadata": {},
   "source": [
    "## join\n",
    "join rejoint deux RDD par clé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac09b099",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize([(\"apple\", 5), (\"orange\", 3)])\n",
    "rdd2 = sc.parallelize([(\"apple\", 2), (\"orange\", 4)])\n",
    "joinedRDD = rdd1.join(rdd2)\n",
    "joinedRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35616ae3",
   "metadata": {},
   "source": [
    "## cogroup\n",
    "cogroup regroupe les valeurs de chaque clé dans deux RDD. Cela peut être utile pour les jointures complexes sur plusieurs RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c990d43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize([(\"apple\", 2), (\"orange\", 3)])\n",
    "rdd2 = sc.parallelize([(\"apple\", 4), (\"orange\", 5)])\n",
    "cogroupedRDD = rdd1.cogroup(rdd2)\n",
    "cogroupedRDD.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
