{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b56c9a92",
   "metadata": {},
   "source": [
    "# Accumulateur Simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d454ce0",
   "metadata": {},
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcfedf1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Spark Context et données\n",
    "sc = spark.sparkContext\n",
    "data = [1, 2, 3, 4, 5, 6, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257e3747",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# creation d'un accumulateur\n",
    "accumulator = sc.accumulator(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1327f81e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Creation d'un RDD\n",
    "numbers = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedf22d6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Fonction d'incrementation de l'accumulateur \n",
    "# il verifie le nombre paire \n",
    "def increment_acc(num):\n",
    "    global accumulator\n",
    "    if num % 2 == 0:\n",
    "        accumulator += 1\n",
    "\n",
    "numbers.foreach(increment_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e2cd07",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"------------ Output --------------\")\n",
    "#affichage de la valeur de l'accumulateur \n",
    "print(\"Nombre d'occurence des nombres paire dans le RDD: \" + str(accumulator.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1592ed2b",
   "metadata": {},
   "source": [
    "# Nous allons créer un accumulateur personnalisé qui regroupe les chaînes et renvoie une chaîne concaténée de toutes les valeurs accumulées :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa67113",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.accumulators import AccumulatorParam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc06e44",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class StringConcatenationAccumulator(AccumulatorParam):\n",
    "    def zero(self, initialValue=\"\"):\n",
    "        return \"\"\n",
    "    \n",
    "    def addInPlace(self, v1, v2):\n",
    "        return v1 + v2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c3e3b7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "customAccumulator = sc.accumulator(\"\", StringConcatenationAccumulator())\n",
    "data = [\"Hello, \", \"this \", \"is \", \"a \", \"custom \", \"accumulator.\"]\n",
    "strings = sc.parallelize(data)\n",
    "strings.foreach(lambda x: customAccumulator.add(x))\n",
    "print(\"------------ Output --------------\")\n",
    "print(\"Concatenated string: \" + customAccumulator.value)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
